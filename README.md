# ğŸ“„ Document AI System

A comprehensive, fully offline document AI system for processing, classifying, extracting, and searching documents using local models and open-source technologies.

## ğŸŒŸ Features

- **ğŸ“¥ Document Ingestion**: Load PDF, DOCX, and TXT files
- **ğŸ·ï¸ Document Classification**: Classify documents into Invoice, Resume, Utility Bill, Other, or Unclassifiable
- **ğŸ” Information Extraction**: Extract structured data based on document type
- **ğŸ§  Vector Embeddings**: Generate embeddings using SentenceTransformers
- **ğŸ” Semantic Search**: Fast similarity search using FAISS
- **â“ Question Answering**: Optional QA using Ollama (local), GROQ API, or local LLM
- **ğŸ“Š Dashboard**: Professional Streamlit dashboard for visualization and interaction

## ğŸ—ï¸ Architecture

The system follows clean architecture principles with modular, layered design:

```
.
â”œâ”€â”€ ingestion/          # Document loading and preprocessing
â”œâ”€â”€ classification/     # Document type classification
â”œâ”€â”€ extraction/         # Structured data extraction
â”œâ”€â”€ embeddings/        # Vector embedding generation
â”œâ”€â”€ retrieval/         # Semantic document retrieval
â”œâ”€â”€ pipelines/         # End-to-end pipeline orchestration
â”œâ”€â”€ qa/                # Question answering (optional)
â”œâ”€â”€ config.py          # Central configuration
â”œâ”€â”€ cli.py             # Command-line interface
â”œâ”€â”€ app.py             # Streamlit dashboard
â””â”€â”€ requirements.txt   # Python dependencies
```

## ğŸ“¦ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Step 1: Clone or Navigate to Project

```bash
cd path/to/Rag
```

### Step 2: Install Dependencies

```bash
# Install all required packages
pip install -r requirements.txt
```

### Step 3: Optional - Setup Question Answering

**Option A: GROQ API (Recommended - Fast & Easy)**

1. Install GROQ package:
   ```bash
   pip install groq
   ```

2. Get API key from [GROQ Console](https://console.groq.com/)

3. Create `.env` file in project root:
   ```
   GROQ_API_KEY=your_api_key_here
   ```

**Option B: Ollama (Recommended - Fully Offline & Easy)**

1. Install Ollama from [ollama.com](https://ollama.com)

2. Pull a model:
   ```bash
   ollama pull llama3.1:8b
   # Or other models: mistral:7b, qwen2.5:7b
   ```

3. Start Ollama (usually runs automatically):
   ```bash
   ollama serve
   ```

4. That's it! The system will automatically detect and use Ollama.

**Option C: Local LLM via llama-cpp-python (Fully Offline)**

1. Install llama-cpp-python:
   ```bash
   pip install llama-cpp-python
   ```

2. Download a GGUF model (e.g., Mistral 7B) from [HuggingFace](https://huggingface.co/TheBloke)

3. Place model in `./models/` directory

## ğŸš€ Running the Program

### Method 1: Streamlit Dashboard (Recommended)

Launch the interactive dashboard:

```bash
streamlit run app.py
```

The dashboard will open in your browser at `http://localhost:8501`

**Dashboard Features:**
- ğŸ“Š Overview metrics and visualizations
- âš™ï¸ Process documents through complete pipeline
- ğŸ” Semantic search interface
- â“ Question answering interface
- ğŸ“ˆ Analytics and insights

### Method 2: Command-Line Interface

#### Process Documents (Complete Pipeline)

```bash
# Process all documents in a directory
python cli.py process --input_dir ./Dataset_files

# Custom output file
python cli.py process --input_dir ./Dataset_files --output results.json
```

#### Classify Documents

```bash
# Classify documents
python cli.py classify Dataset_files

# Save results to JSON
python cli.py classify Dataset_files --output classifications.json
```

#### Extract Information

```bash
# Extract structured data
python cli.py extract Dataset_files

# Save results
python cli.py extract Dataset_files --output extractions.json
```

#### Generate Embeddings

```bash
# Generate embeddings
python cli.py embed Dataset_files

# Save to file
python cli.py embed Dataset_files --output embeddings.json
```

#### Index Documents

```bash
# Index documents for search
python cli.py index Dataset_files

# Custom store path
python cli.py index Dataset_files --store-path ./my_index
```

#### Search Documents

```bash
# Semantic search
python cli.py search --query "invoice payment terms" --top-k 5

# Custom store path
python cli.py search --query "resume skills" --store-path ./my_index --top-k 10
```

#### Question Answering

```bash
# Using Ollama (recommended - fully offline)
python cli.py qa --provider ollama --ollama-model llama3.1:8b --question "Find all documents mentioning payments due in January" --top-k 5

# Using GROQ API (fast, requires API key)
python cli.py qa --provider groq --question "What is the total amount on invoice_1?" --top-k 3

# Using local LLM (GGUF model)
python cli.py qa --provider local --question "What are the payment terms?" --model-path ./models/mistral.gguf

# Auto-detect provider (tries Ollama first, then GROQ, then local LLM)
python cli.py qa --question "What is the total amount on invoice_1?" --top-k 3
```

## ğŸ“š Libraries and Methods Used

### Core Dependencies

#### Document Processing
- **PyPDF2** (â‰¥3.0.0): PDF text extraction
- **python-docx** (â‰¥1.1.0): DOCX document parsing

#### Machine Learning & NLP
- **sentence-transformers** (â‰¥2.2.0): 
  - Model: `all-MiniLM-L6-v2` (384-dimensional embeddings)
  - Used for: Document classification and embedding generation
- **torch** (â‰¥2.0.0): PyTorch backend for sentence-transformers
- **scikit-learn** (â‰¥1.3.0): Utility functions for ML operations
- **numpy** (â‰¥1.24.0): Numerical operations and array handling

#### Vector Search
- **faiss-cpu** (â‰¥1.7.4): 
  - Fast similarity search using FAISS (Facebook AI Similarity Search)
  - Index type: Flat index with cosine similarity
  - Used for: Efficient document retrieval

#### Question Answering (Optional)
- **Ollama** (Recommended for local QA):
  - Local LLM server (install from [ollama.com](https://ollama.com))
  - Models: Llama 3.1, Mistral, Qwen, and more
  - Used for: Fully offline question answering (easiest setup)
- **groq** (â‰¥0.4.0): 
  - GROQ API client for fast inference
  - Models: Llama 3.1 70B, Llama 3 70B/8B
  - Used for: Cloud-based question answering (fastest)
- **llama-cpp-python** (optional): 
  - Local LLM inference via LLaMA.cpp
  - GGUF model format
  - Used for: Fully offline question answering (advanced)
- **requests** (â‰¥2.31.0):
  - HTTP client for Ollama API communication

#### Dashboard
- **streamlit** (â‰¥1.28.0): Web dashboard framework
- **streamlit-option-menu** (â‰¥0.3.6): Navigation menu component
- **plotly** (â‰¥5.17.0): Interactive visualizations
- **pandas** (â‰¥2.0.0): Data manipulation and analysis

#### Utilities
- **python-dotenv** (â‰¥1.0.0): Environment variable management
- **tqdm** (â‰¥4.65.0): Progress bars
- **typing-extensions** (â‰¥4.5.0): Enhanced type hints

### Methods and Techniques

#### 1. Document Classification
- **Primary Method**: SentenceTransformers embeddings + cosine similarity
  - Uses pre-trained `all-MiniLM-L6-v2` model
  - Class prototypes: Short reference texts for each category
  - Cosine similarity between document and class embeddings
- **Fallback Method**: Keyword-based heuristics
  - Regex pattern matching for document-specific keywords
  - Confidence thresholding (0.5 for ML, 0.3 for keywords)
- **Output**: Classification with confidence score

#### 2. Information Extraction
- **Rule-based Extraction**: Regex patterns for structured data
- **Date Normalization**: Multiple format support (ISO, MM/DD/YYYY, etc.)
- **Currency Extraction**: Pattern matching for monetary values
- **Type-specific Extractors**:
  - Invoice: invoice_number, date, company, total_amount
  - Resume: name, email, phone, experience_years
  - Utility Bill: account_number, date, usage_kwh, amount_due

#### 3. Embedding Generation
- **Model**: SentenceTransformers `all-MiniLM-L6-v2`
- **Dimension**: 384
- **Normalization**: L2 normalization for cosine similarity
- **Caching**: Disk-based caching for deterministic results
- **Document IDs**: SHA256 hash-based for unique identification

#### 4. Vector Search
- **Index**: FAISS Flat Index
- **Similarity Metric**: Cosine similarity (normalized vectors)
- **Search Method**: Exact nearest neighbor search
- **Storage**: Local disk-based with metadata persistence

#### 5. Question Answering
- **Ollama** (Recommended - Fully Offline):
  - Local LLM server running on `http://localhost:11434`
  - Models: Llama 3.1 (8B/70B), Mistral 7B, Qwen 2.5, and more
  - Easy setup: Just install Ollama and pull a model
  - Fully offline, no API keys needed
  - Automatic model management
- **GROQ API** (Fast - Cloud-based):
  - Models: Llama 3.1 70B, Llama 3 70B/8B
  - Fast inference with high-quality responses
  - Context window: Up to 128K tokens (Llama 3.1)
  - Requires API key
- **Local LLM via llama-cpp-python** (Advanced - Fully Offline):
  - LLaMA.cpp backend
  - GGUF model format
  - Manual model download and setup required

## ğŸ“‹ Project Structure

```
Rag/
â”œâ”€â”€ ingestion/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ loader.py          # PDF, DOCX, TXT loaders
â”‚   â””â”€â”€ preprocessor.py    # Text cleaning and chunking
â”œâ”€â”€ classification/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ classifier.py     # Document classification
â”œâ”€â”€ extraction/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ extractor.py      # Structured data extraction
â”œâ”€â”€ embeddings/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ generator.py       # Embedding generation
â”‚   â””â”€â”€ store.py          # FAISS vector store
â”œâ”€â”€ retrieval/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ retriever.py      # Semantic search
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ document_pipeline.py  # Complete processing pipeline
â”‚   â”œâ”€â”€ ingestion_pipeline.py
â”‚   â””â”€â”€ rag_pipeline.py
â”œâ”€â”€ qa/                    # Optional question answering
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ answerer.py       # Local LLM QA (llama-cpp-python)
â”‚   â”œâ”€â”€ groq_answerer.py # GROQ API QA
â”‚   â””â”€â”€ ollama_answerer.py # Ollama local QA (recommended)
â”œâ”€â”€ config.py             # Configuration management
â”œâ”€â”€ cli.py                # Command-line interface
â”œâ”€â”€ app.py                # Streamlit dashboard
â”œâ”€â”€ requirements.txt      # Dependencies
â””â”€â”€ README.md            # This file
```

## ğŸ”§ Configuration

Configuration is managed through `config.py`. Key settings:

- **Classification**: Model name, confidence thresholds
- **Embedding**: Model name, device (CPU/CUDA), batch size
- **Vector Store**: Store path, similarity metric
- **Retrieval**: Top-K, similarity threshold

## ğŸ“Š Output Format

### Assessment Output (`output.json`)

The main output file follows the assessment specification format:

```json
{
  "invoice_1.pdf": {
    "class": "Invoice",
    "invoice_number": "1001",
    "date": "2025-06-16",
    "company": "Pioneer Ltd",
    "total_amount": 2073.0
  },
  "resume_1.pdf": {
    "class": "Resume",
    "name": "John Doe",
    "email": "john.doe@example.com",
    "phone": "+1-555-799-6125",
    "experience_years": 5
  },
  "utilitybill_1.pdf": {
    "class": "Utility Bill",
    "account_number": "ACC-12345",
    "date": "2025-01-15",
    "usage_kwh": 1250.5,
    "amount_due": 150.75
  }
}
```

### Full Processing Results (`output_full.json`)

A detailed output file with complete metadata is also generated:

```json
[
  {
    "document_id": "abc123...",
    "file_path": "path/to/document.pdf",
    "file_name": "document.pdf",
    "text": "Extracted text content...",
    "classification": {
      "class": "Invoice",
      "confidence": 0.85
    },
    "extracted_data": {
      "invoice_number": "INV-001",
      "date": "2025-01-15",
      "company": "Example Corp",
      "total_amount": 1500.00
    },
    "embedding": {
      "dimension": 384,
      "vector": [0.123, -0.456, ...]
    },
    "status": "success",
    "error": null
  }
]
```

### Search Results

```json
[
  {
    "document_id": "abc123...",
    "score": 0.9234,
    "snippet": "Relevant text snippet...",
    "file_name": "document.pdf"
  }
]
```

## ğŸ¯ Use Cases

1. **Document Management**: Automatically classify and organize documents
2. **Data Extraction**: Extract structured information from unstructured documents
3. **Semantic Search**: Find documents by meaning, not just keywords
4. **Question Answering**: Ask questions about your document collection
5. **Analytics**: Analyze document types, confidence scores, and extraction rates

## ğŸ”’ Privacy & Security

- **Fully Offline**: Core features work without internet connection
- **Local Processing**: All document processing happens locally
- **No Data Sharing**: Documents never leave your machine
- **Optional Cloud**: GROQ API is optional and clearly indicated

## ğŸ› Troubleshooting

### Common Issues

**Issue**: "No module named 'sentence_transformers'"
- **Solution**: `pip install sentence-transformers torch`

**Issue**: "FAISS not found"
- **Solution**: `pip install faiss-cpu`

**Issue**: PDF reading errors
- **Solution**: PDF may be corrupted. Check file integrity.

**Issue**: GROQ API not working
- **Solution**: Verify `GROQ_API_KEY` is set in `.env` file

**Issue**: Ollama not detected
- **Solution**: Ensure Ollama is running (`ollama serve`) and a model is pulled (`ollama pull llama3.1:8b`)

## ğŸ“ License

[Add your license here]

## ğŸ¤ Contributing

[Add contribution guidelines if applicable]

## ğŸ“§ Contact

[Add contact information if needed]

---

**Built with**: Python, SentenceTransformers, FAISS, Streamlit, Ollama, GROQ API
